{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial_Sleep_Staging_B.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"25mYtkZbNgnq"},"source":["Launch the following blocks to connect to your drive and go into the tutorial folder"]},{"cell_type":"code","metadata":{"id":"XpSZkaqFDrRN"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pokgRnDVDxzZ"},"source":["%cd /content/gdrive/My\\ Drive/TD_Dreem_MasterBin/Dreem_Master_Bin\n","! ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Br682vIyDkCy"},"source":["This tutorial is about machine learning methods for sleep stage classification.\n"]},{"cell_type":"code","metadata":{"id":"ZONWWYI-hKl6","executionInfo":{"status":"ok","timestamp":1603287838891,"user_tz":-120,"elapsed":3195,"user":{"displayName":"Karim el Kanbi","photoUrl":"","userId":"15502708993880309867"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","from dreem_master_bin.hypnogram import plot_hypnogram, stage_colors"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnHoNfP4hLUl"},"source":["To save you some time, train and test datasets are already available in the data folder. It consists of preprocessed data, not raw record. You have two types of datasets:\n","\n","- Spectral dataset = data containing spectral power (spectrogram matrix): train and test\n","- Features dataset = data containing precomputed features: train and test\n","\n","As with real-life data, we will not be allowed to use test data for training. The test data will only be used for the final evaluation."]},{"cell_type":"code","metadata":{"id":"KfzUJqPPboAi","executionInfo":{"status":"ok","timestamp":1603287844956,"user_tz":-120,"elapsed":4980,"user":{"displayName":"Karim el Kanbi","photoUrl":"","userId":"15502708993880309867"}}},"source":["from dreem_master_bin.load_data import load_spectral_datasets, load_feature_datasets\n","\n","# load spectrogram dataset and shuffle train data\n","x_train_spect, y_train_spect, x_test_spect, y_test_spect = load_spectral_datasets()\n","spectral_names = ['index_window', '1Hz', '2Hz', '3Hz', '4Hz', '5Hz', '6Hz', '7Hz',\n","                  '8Hz', '9Hz', '10Hz', '11Hz', '12Hz', '13Hz', '14Hz', '15Hz', \n","                  '16Hz', '18Hz', '19Hz']\n","# shuffle train dataset\n","p = np.random.permutation(len(y_train_spect))\n","x_train_spect, y_train_spect = x_train_spect[p], y_train_spect[p]\n","\n","\n","# load precomputed features dataset and shuffle train data\n","x_train_feat, y_train_feat, x_test_feat, y_test_feat = load_feature_datasets()\n","features_name = ['index_window', 'delta', 'delta_r', 'theta', 'theta_r',\n","       'lowfreq', 'lowfreq_r', 'alpha', 'alpha_r', 'sigma', 'sigma_r', 'beta',\n","       'beta_r', 'kcomp', 'kcomp_r', 'SC', 'SEF90', 'SEF95', 'Nb spindles',\n","       'spindles magnitude', 'spindles duration', 'Nb slow waves',\n","       'slow waves magnitude', 'slow waves duration', 'AccelerometerVar',\n","       'little movement', 'strong movement']\n","# shuffle train dataset\n","p = np.random.permutation(len(y_train_feat))\n","x_train_feat, y_train_feat = x_train_feat[p], y_train_feat[p]\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMXTLgSVb-Vk"},"source":["We have just loaded the spectral data:\n","\n","- x_train_spect, x_test_spect: spectral data to predict sleep stages\n","It is an array of shape n_samples x n_features\n","\n","    - n_samples = number of sleep epochs\n","    - n_features = number of features for each of these epochs. The features are: [index_window, power_frequency_1Hz, power_frequency_2Hz, ..., power_frequency_18Hz], where index window to the position of the sample in its sleep record.\n","- y_train_spect, y_test_spect: labels (sleep stages)\n","\n","Then, we have loaded the other dataset:\n","\n","- x_train_spect, x_test_spect: shape n_samples x n_features\n","    - n_features = the features are ['index_window', 'delta', 'delta_r', 'theta', 'theta_r', 'lowfreq', 'lowfreq_r', 'alpha', 'alpha_r', 'sigma', 'sigma_r', 'beta', 'beta_r', 'kcomp', 'kcomp_r', 'SC', 'SEF90', 'SEF95', 'Nb spindles', 'spindles magnitude', 'spindles duration', 'Nb slow waves', 'slow waves magnitude', 'slow waves duration', 'AccelerometerVar','little movement', 'strong movement']\n","\n","- y_train_spect, y_test_spect: labels (sleep stages)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"riBnDit20rtG"},"source":["We need also to define a function to will evaluate the models"]},{"cell_type":"code","metadata":{"id":"Gs_poN6u0qJ6","executionInfo":{"status":"ok","timestamp":1603287848581,"user_tz":-120,"elapsed":869,"user":{"displayName":"Karim el Kanbi","photoUrl":"","userId":"15502708993880309867"}}},"source":["def evaluate(model, x_data, true_labels):\n","    predictions = model.predict(x_data)\n","    scores = {'balanced_accuracy': balanced_accuracy_score(true_labels, predictions),\n","            'cohen_kappa': cohen_kappa_score(true_labels, predictions),\n","            'confusion_matrix': confusion_matrix(true_labels, predictions)}\n","\n","    return scores"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68gD5UbGEDod"},"source":["Let's start with the spectral dataset !"]},{"cell_type":"code","metadata":{"id":"uZcvEkUvDjWX","executionInfo":{"status":"ok","timestamp":1603287851146,"user_tz":-120,"elapsed":672,"user":{"displayName":"Karim el Kanbi","photoUrl":"","userId":"15502708993880309867"}}},"source":["x_train, y_train = x_train_spect, y_train_spect\n","x_test, y_test = x_test_spect, y_test_spect"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jv-Rft5PEcHk"},"source":["1 - Feature scaling + Dimension reduction + classifier\n","\n","It is crucial to process your raw data before inserting them in a classifier. \n","In this section, you will work with data that have already been processed (spectral and features dataset). Then, you can:\n","- scale your data (here we are using StandardScaler)\n","- apply a dimension reduction (PCA, ICA...)\n","\n","After that you can choose your classifier (e.g SVM classifier)\n","\n","\n","*You can go to the online documentation of the scikit-library to find similar functions, with the keywords:* \n","- *multi class classifier*\n","- *dimension reduction*\n","- *decomposition*\n"]},{"cell_type":"code","metadata":{"id":"Ekji3UiPEO5_"},"source":["from sklearn.decomposition import PCA\n","from sklearn.svm import SVC\n","\n","# scale input data and reduce dimension\n","pca = make_pipeline(StandardScaler(),\n","                    PCA(n_components=5, random_state=10))\n","pca.fit(x_train, y_train)\n","\n","# linear classifier\n","classifier = SVC(kernel='linear')\n","# training: fit the model to the data\n","print('training')\n","classifier.fit(pca.transform(x_train), y_train)\n","\n","# evaluation\n","evaluate(classifier, pca.transform(x_test), y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rlsbk_l8bRI_"},"source":["You can see that we predict almost no N1 stage: this may be due to the class distribution inside the dataset. Let's have a look on the train and test datasets:"]},{"cell_type":"code","metadata":{"id":"3MpEnLQZbqbQ"},"source":["# percentage of sleep stages in y_train and y_test\n","stage_correspondance = {0: \"WAKE\", 1: \"N1\", 2: \"N2\", 3: \"DEEP\", 4: \"REM\"}\n","\n","print('train dataset')\n","for k, stage_name in stage_correspondance.items():\n","    percentage = round(100 * sum(y_train==k) / len(y_train), 2)\n","    print(stage_name + ': ' + str(percentage) + '%')\n","\n","print('')\n","print('test dataset')\n","for k, stage_name in stage_correspondance.items():\n","    percentage = round(100 * sum(y_test==k) / len(y_test), 2)\n","    print(stage_name + ': ' + str(percentage) + '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"td4EOQPaeCIW"},"source":["You can notice that the datasets are unbalanced. Indeed, as they were made from whole sleep records, the distribution of sleep stages in the dataset is characteristic of the average percentage of each sleep stage during a night sleep."]},{"cell_type":"markdown","metadata":{"id":"7---cUpLE-lN"},"source":["2 - Tune your model\n","\n","\n","Here we are going to use the Random Forest classifier, one of the many ensemble learning functions of the scikit-learn library: https://scikit-learn.org/stable/modules/ensemble.html\n","\n","Also, we are going to work with the preprocessed features dataset. \n"]},{"cell_type":"code","metadata":{"id":"Fp7B5w_GHhYH"},"source":["# load features dataset and shuffle train data\n","x_train, y_train = x_train_feat, y_train_feat\n","x_test, y_test = x_test_feat, y_test_feat\n","\n","# select a classifier and train it\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rf = make_pipeline(StandardScaler(),\n","                       RandomForestClassifier())\n","print('training...')\n","rf.fit(x_train, y_train)\n","\n","# evaluation\n","evaluate(rf, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1OJHkqq5llHO"},"source":["If you look at the documentation of RandomForestClassifier, you will see that it has many arguments. Most of them are hyper-parameters. The training process will only change the parameters of the model, but it is up to you to find the best hyper-parameters."]},{"cell_type":"code","metadata":{"id":"2mNQWDAiq-xs"},"source":["from pprint import pprint\n","# Look at parameters used by our current forest\n","print('Parameters currently in use:\\n')\n","pprint(rf.get_params()['randomforestclassifier'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_j2vhoZtNMV"},"source":["We will try adjusting the following set of hyperparameters:\n","\n","- n_estimators = number of trees in the foreset\n","- max_features = max number of features considered for splitting a node\n","- max_depth = max number of levels in each decision tree\n","- min_samples_split = min number of data points placed in a node before the node is split\n","- min_samples_leaf = min number of data points allowed in a leaf node\n","- bootstrap = method for sampling data points (with or without replacement)\n","\n","To do this, we are going to perform a randomized search (with cross-validation) using RandomizedSearchCV. RandomizedSearchCV peerforms a randomized search in the hyperparameter space to find the best model, it goes then faster than a complete search over all the combinations of parameters.\n","\n","The number of fit permormed is equal to cv * n_iter, each fit takes between 1 and 3 minutes."]},{"cell_type":"code","metadata":{"id":"qRg_LtFotL-8"},"source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","# Number of trees in random forest\n","n_estimators = [int(x) for x in np.linspace(start = 800, stop = 1600, num = 5)]\n","# Number of features to consider at every split\n","max_features = ['auto', 'sqrt']\n","# Maximum number of levels in tree\n","max_depth = [int(x) for x in np.linspace(20, 30, num = 3)]\n","max_depth.append(None)\n","# Minimum number of samples required to split a node\n","min_samples_split = [5, 10]\n","# Minimum number of samples required at each leaf node\n","min_samples_leaf = [1, 2, 4]\n","# Method of selecting samples for training each tree\n","bootstrap = [True, False]\n","# Create the random grid\n","random_grid = {'n_estimators': n_estimators,\n","               'max_features': max_features,\n","               'max_depth': max_depth,\n","               'min_samples_split': min_samples_split,\n","               'min_samples_leaf': min_samples_leaf,\n","               'bootstrap': bootstrap}\n","pprint(random_grid)\n","\n","\n","# Use the random grid to search for best hyperparameters\n","# First create the base model to tune\n","rf = RandomForestClassifier()\n","# Random search of parameters, using 3 fold cross validation, \n","# search across 100 different combinations, and use all available cores\n","rf_random = RandomizedSearchCV(estimator=rf, \n","                               param_distributions=random_grid, \n","                               n_iter=3, # Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.\n","                               cv=3, # Determines the cross-validation splitting strategy\n","                               verbose=2, \n","                               random_state=42, \n","                               n_jobs = 8)\n","# Fit the random search model\n","rf_random.fit(x_train, y_train)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s9I-TkPrxYgf"},"source":["Have a look at this article during the fit: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74 \n","\n","It explains the concept behind RandomizedSearchCV:\n","- randomized search over a space of hyperparameters\n","- Cross-validation: if you only work with a training dataset, you increase your chance of overfitting."]},{"cell_type":"code","metadata":{"id":"YEY8OuCetG3j"},"source":["# look at the best params computed from RandomizedSearchCV\n","print('Best parameters from RandomizedSearCV')\n","pprint(rf_random.best_params_)\n","\n","print('')\n","print('Base model: default value for hyperparameters')\n","base_model = rf.fit(x_train, y_train)\n","base_accuracy = evaluate(base_model, x_test, y_test)\n","pprint(base_accuracy)\n","\n","print('')\n","print('Best model from RandomizedSearCV')\n","best_random = rf_random.best_estimator_\n","random_accuracy = evaluate(best_random, x_test, y_test)\n","pprint(random_accuracy)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0nHioKyTpdz"},"source":["3 - Stack multiple estimators\n","\n","It is possible to combine multiple machine learning algorithms to improve performance.\n","\n","> Use StackingClassifier to stack estimators with a final classifier\n"]},{"cell_type":"code","metadata":{"id":"sxRGV5fRTvx5"},"source":["# select a classifier and train it\n","from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.linear_model import LogisticRegression\n","\n","rf_pipeline = make_pipeline(StandardScaler(),\n","                            RandomForestClassifier(n_estimators=10, random_state=42))\n","gradient_pipeline = make_pipeline(StandardScaler(),\n","                                  HistGradientBoostingClassifier(learning_rate=0.01, random_state=30))\n","estimators = [('Random Forest', rf_pipeline),\n","                  ('Gradient Boosting', gradient_pipeline)]\n","stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=200))\n","\n","print('training...')\n","stacking_classifier.fit(x_train, y_train)\n","\n","# evaluation\n","evaluate(rf, x_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kX_vxnx16b8p"},"source":["4 - Feature importance\n","\n","In all this tutorial, we have tried to predict sleep stages from precomputed features (spectral or other features). \n","\n","Scikit provides methods to assess to importance of each feature for the prediction."]},{"cell_type":"code","metadata":{"id":"oAfKM6ys7wPb"},"source":["# load spectrogram dataset and shuffle train data\n","x_train, y_train = x_train_feat, y_train_feat\n","list_features = features_name\n","\n","# let's take an already trained classifier\n","clf = stacking_classifier\n","\n","# permutation importance > feature importance\n","print('permutations...')\n","from sklearn.inspection import permutation_importance\n","result = permutation_importance(clf, x_train, y_train, n_repeats=10, random_state=0)\n","\n","# sort by importance\n","sorted_idx = result.importances_mean.argsort()\n","\n","# Plot\n","fig, ax = plt.subplots(figsize=(25, 10))\n","ax.boxplot(result.importances[sorted_idx].T,\n","           vert=False, labels=[list_features[i] for i in sorted_idx])\n","ax.set_title(\"Permutation Importances (train set)\")\n","fig.tight_layout()\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yjnFmonvZL0x"},"source":["You've reached the end of this second tutorial.\n","\n","Let's go to the last part about deep learning methods.\n","Open the **Tutorial_Sleep_Staging_C** tutorial"]}]}