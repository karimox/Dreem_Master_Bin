{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport h5py # Read and write HDF5 files from Python\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going into the Dreem 3 Challenge.\nThe goal is to use Dreem 3 headband data to perform sleep stage scoring on 30 seconds epochs of biophysiological signals.\nhttps://www.kaggle.com/c/dreem-3-sleep-classification-challenge-2021/data\n\nThe training dataset is composed of:\n- X_train.h5: input Dreem3 headband data: 30s of biosignals including EEG and accelerometer\n- y_train: sleep stages {'Wake':0, 'N1':1, 'N2':2, 'N3':3, 'REM':4} \n\nThe challenge is to submit the sleep stages associated to:\n- X_test.h5\n(it has to be submitted in the right format, see sample_submission.csv)","metadata":{}},{"cell_type":"code","source":"# filenames\ndata_path = \"/kaggle/input/dreem-3-sleep-classification-challenge-2021/\"\nfile_xtrain = data_path + \"X_train.h5\"\nfile_xtest = data_path + \"X_test.h5\"\nfile_ytrain = data_path + \"y_train.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the data","metadata":{}},{"cell_type":"code","source":"# training labels\npd.read_csv(file_ytrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what does the h5 file contains ?\nwith h5py.File(file_xtrain, \"r\") as hf:\n        print(list(hf.keys()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How to load data from h5? what is its shape and type?\nwith h5py.File(file_xtrain, \"r\") as hf:\n        field = list(hf.keys())[0]\n        x_data = hf[field][()]\ntype(x_data), x_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this TD, we will only work with one EEG channel.\nLet's create dataset functions that will be used for training and testing the model:\n\n*EegEpochDataset*: Eeg Class herited from pytorch Dataset to deal with our data\n\n*get_train_validation_dataset*: \n- return train_dataloader and validation_dataloader\n- dataloaders will be used during the training and the tests","metadata":{}},{"cell_type":"code","source":"\"\"\" Load project data\n    DataLoader and Dataset for single-channel EEG\n\n\"\"\"\n\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef normalize_data(eeg_array):\n    \"\"\"normalize signal between 0 and 1\"\"\"\n\n    normalized_array = np.clip(eeg_array, -150, 150)\n    normalized_array = normalized_array / 150\n\n    return normalized_array\n\n\nclass EegEpochDataset(Dataset):\n    \"\"\"EEG Epochs dataset.\"\"\"\n\n    def __init__(self, x_data, y_data, transform=None):\n        \"\"\"\n        Args:\n            x_data (numpy array): Numpy array of input data.\n            y_data (list of numpy array): Sleep Stages\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.y_data = y_data\n        self.x_data = x_data\n        self.transform = transform\n\n        self.x_data = normalize_data(x_data)\n\n    def __len__(self):\n        return len(self.y_data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        signal = np.expand_dims(self.x_data[idx], axis=0)\n        stage = self.y_data[idx]\n\n        if self.transform:\n            signal = self.transform(signal)\n\n        return signal, stage\n    \n\ndef get_train_validation_dataset(derivation, validation_ratio=0.2):\n    \"\"\"\n    Return train and validation datasets in Dataloader format\n    :param derivation: EEG derivation, in [eeg_1,eeg_2, eeg_4, eeg_5, eeg_6]\n    :param batch_size: size of the batch, usually 16, 32 or 64\n    :param validation_ratio:\n\n    :return:\n    train_dataloader\n    validation_dataloader\n    \"\"\"\n\n    with h5py.File(file_xtrain, \"r\") as fi:\n        x_data = fi[derivation][()]\n    y_data = pd.read_csv(file_ytrain)['sleep_stage'].to_numpy()\n\n    # Creating data indices for training and validation splits:\n    dataset_size = len(y_data)\n    indices = list(range(dataset_size))\n    split = int((1 - validation_ratio) * dataset_size)\n    np.random.shuffle(indices)\n    train_indices, val_indices = indices[:split], indices[split:]\n\n    x_train, x_validation = x_data[train_indices], x_data[val_indices]\n    y_train, y_validation = y_data[train_indices], y_data[val_indices]\n\n    # torch dataset\n    train_dataset = EegEpochDataset(x_data=x_train, y_data=y_train)\n    val_dataset = EegEpochDataset(x_data=x_validation, y_data=y_validation)\n\n\n    return train_dataset, val_dataset\n\n\n# load dataloaders - final_val is the dataset for the last validation\ntrain_dataset, final_val_dataset = get_train_validation_dataset('eeg_1')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we create the neural network Model:\n- convolutionnal neural network\n- Fully conencted layers at the end\n- takes only a single channel of EEG signal as input","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SingleChannelConvNet(nn.Module):\n\n    def __init__(self):\n        super(SingleChannelConvNet, self).__init__()\n        # convolutionnal mayers\n        self.conv_a = nn.Conv1d(1, 128, 7, stride=2, padding=6, padding_mode='zeros')\n        self.conv_b = nn.Conv1d(128, 128, 7, stride=2, padding=6, padding_mode='zeros')\n        self.conv_c = nn.Conv1d(128, 256, 7, stride=2, padding=6, padding_mode='zeros')\n        self.conv_d = nn.Conv1d(256, 256, 5, stride=2, padding=4, padding_mode='zeros')\n        self.conv_e = nn.Conv1d(256, 256, 3, stride=2, padding=2, padding_mode='zeros')\n\n        # pool layers\n        self.pool = nn.MaxPool1d(2)\n\n        # non linearity\n        self.activfunc_a = nn.LeakyReLU(negative_slope=0.1)\n\n        # fully connected layers - at the end\n        self.fc1 = nn.Linear(3 * 256, 100)\n        self.fc2 = nn.Linear(100, 5)\n\n    def forward(self, x):\n\n        x = self.activfunc_a(self.conv_a(x))\n        for _ in range(5):\n            x = self.activfunc_a(self.conv_b(x))\n        x = self.activfunc_a(self.conv_c(x))\n        for _ in range(3):\n            x = self.activfunc_a(self.conv_d(x))\n        x = self.activfunc_a(self.conv_e(x))\n        x = self.activfunc_a(self.conv_e(x))\n\n        x = x.view(-1, self.num_flat_features(x)) # flatten the tensor\n        x = self.activfunc_a(self.fc1(x))\n        x = self.fc2(x)\n\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can now start the training on the train dataloader:\n- model will train many times on the dataset: n_epochs\n- training dataset will be split in three subset (k_fold cross-validation)\n- loss_val: mean loss on the validation datasets, computed after each epochs of training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# device: use GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# parameters\nlearning_rate = 0.001\nn_epoch = 20\nk_fold = 3\nbatch_size = 32\n\n# neural network\nmy_net = SingleChannelConvNet()\nmy_net = my_net.to(device) # model into GPU\n# loss function\ncriterion = nn.CrossEntropyLoss()\n# optimisation algorithm \noptimizer = optim.Adam(my_net.parameters(), lr=learning_rate)\n\n\n# function: evaluate the loss of validation subset\ndef loss_val(net, val_loader):\n    with torch.no_grad(): # do not forget to remove gradient computing during evaluation !!!\n        val_loss = 0.0\n        for data in val_dataloader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n        return val_loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start the loop !","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom torch.utils.data.dataset import Subset\nfrom torch.utils.data import DataLoader\n\n# List all the validation loss:\n# at the end of each epoch of training, a loss is computed on a subset of data\nall_val_loss = []\n\nprint('training...')\nfor epoch in range(n_epoch):  # loop over the dataset multiple times\n\n    # validation losses for this epoch (n=k_fold)\n    val_loss = []        \n    for train_indices, val_indices in KFold(n_splits=k_fold).split(list(range(len(train_dataset)))):\n        # k_fold dataloader (k=3) - Take validation subset for training, to avoid overfit\n        train_subset = Subset(train_dataset, train_indices)\n        val_subset = Subset(train_dataset, val_indices)\n\n        train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n        val_dataloader = DataLoader(val_subset, batch_size=batch_size, num_workers=8)\n\n        running_loss = 0.0\n        for i, data in enumerate(train_dataloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + loss + backward + optimize\n            outputs = my_net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            if i % 100 == 99:\n                print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss))\n            running_loss = 0.0\n\n        # average validation losses\n        val_loss += [loss_val(my_net, val_dataloader)]\n        \n    all_val_loss += [np.round(np.mean(val_loss), 2)]\n    print(all_val_loss)\n\n    \nprint('Finished Training')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\nfrom sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, f1_score, confusion_matrix\n\n# score function\ndef evaluate(true, pred):\n    scores = {'balanced_accuracy': balanced_accuracy_score(true, pred),\n            'cohen_kappa': cohen_kappa_score(true, pred),\n            'confusion_matrix': confusion_matrix(true, pred),\n            'f1_score': f1_score(true, pred, average=\"macro\")}\n\n    return scores\n\n# params\nclasses = ['Wake', 'N1', 'N2', 'N3', 'REM']\n\n# final validation dataset: has not be used for the training\nval_dataloader = DataLoader(final_val_dataset, batch_size=batch_size, num_workers=8)\n\n# evaluate the performance of the model\nwith torch.no_grad():\n    prediction_list = torch.empty(0).to(device)\n    true_list = torch.empty(0).to(device)\n    for data in val_dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = my_net(inputs)\n        _, predicted = torch.max(outputs, 1)\n        prediction_list = torch.cat([prediction_list, predicted])\n        true_list = torch.cat([true_list, labels])\n\n        \n# Scores\ntrue_list = true_list.cpu().numpy()\nprediction_list = prediction_list.cpu().numpy()\nscores = evaluate(true_list, prediction_list)\n\nprint(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"During the training, you may have noticed that you could have stopped earlier to have a lower validation, and maybe a better model at the end.\nRewrite the code to save the 3 models with the lower validation loss, and compare them on the final_validation_dataset !","metadata":{}}]}