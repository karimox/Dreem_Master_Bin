{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Tutorial_Sleep_Staging_C.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"id":"OK2WjGZ6xim0"},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import h5py # Read and write HDF5 files from Python\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ssg591Wxim_"},"source":["Now we are going into the Dreem 2 Challenge.\n","The goal is to use Dreem 2 headband data to perform sleep stage scoring on 30 seconds epochs of biophysiological signals.\n","https://www.kaggle.com/c/ei-dreem-sleep-stages-2020/data\n","\n","The training dataset is composed of:\n","- X_train.h5: input Dreem2 headband data: 30s of biosignals including EEG and accelerometer\n","- y_train: sleep stages {'Wake':0, 'N1':1, 'N2':2, 'N3':3, 'REM':4} \n","\n","The challenge is to submit the sleep stages associated to:\n","- X_test.h5\n","(it has to be submitted in the right format, see sample_submission.csv)\n"]},{"cell_type":"code","metadata":{"trusted":true,"id":"8MHxahAYxinA"},"source":["# filenames\n","data_path = \"/kaggle/input/dreem-2-sleep-classification-challenge-2020/\"\n","file_xtrain = data_path + \"X_train.h5/X_train.h5\"\n","file_xtest = data_path + \"X_test.h5/X_test.h5\"\n","file_ytrain = data_path + \"y_train.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJ8wdSk6xinJ"},"source":["Let's have a look at the data"]},{"cell_type":"code","metadata":{"trusted":true,"id":"2Y2M2IwOxinK"},"source":["# training labels\n","pd.read_csv(file_ytrain)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"pQyZXYWUxinR"},"source":["# what does the h5 file contains ?\n","with h5py.File(file_xtrain, \"r\") as hf:\n","        print(list(hf.keys()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"dg0KdddhxinW"},"source":["# How to load data from h5? what is its shape and type?\n","with h5py.File(file_xtrain, \"r\") as hf:\n","        field = list(hf.keys())[0]\n","        x_data = hf[field][()]\n","type(x_data), x_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4JPBGU0xind"},"source":["In this TD, we will only work with one EEG channel.\n","Let's create dataset functions that will be used for training and testing the model:\n","\n","*EegEpochDataset*: Eeg Class herited from pytorch Dataset to deal with our data\n","\n","*get_train_validation_dataset*: \n","- return train_dataloader and validation_dataloader\n","- dataloaders will be used during the training and the tests\n"]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"uQOtPHbUxine"},"source":["\"\"\" Load project data\n","    DataLoader and Dataset for single-channel EEG\n","\n","\"\"\"\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","def normalize_data(eeg_array):\n","    \"\"\"normalize signal between 0 and 1\"\"\"\n","\n","    normalized_array = np.clip(eeg_array, -150, 150)\n","    normalized_array = normalized_array / 150\n","\n","    return normalized_array\n","\n","\n","class EegEpochDataset(Dataset):\n","    \"\"\"EEG Epochs dataset.\"\"\"\n","\n","    def __init__(self, x_data, y_data, transform=None):\n","        \"\"\"\n","        Args:\n","            x_data (numpy array): Numpy array of input data.\n","            y_data (list of numpy array): Sleep Stages\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.y_data = y_data\n","        self.x_data = x_data\n","        self.transform = transform\n","\n","        self.x_data = normalize_data(x_data)\n","\n","    def __len__(self):\n","        return len(self.y_data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        signal = np.expand_dims(self.x_data[idx], axis=0)\n","        stage = self.y_data[idx]\n","\n","        if self.transform:\n","            signal = self.transform(signal)\n","\n","        return signal, stage\n","    \n","\n","def get_train_validation_dataset(derivation, validation_ratio=0.2):\n","    \"\"\"\n","    Return train and validation datasets in Dataloader format\n","    :param derivation: EEG derivation, from eeg_1 to eeg_7\n","    :param batch_size: size of the batch, usually 16, 3Ã© or 64\n","    :param validation_ratio:\n","\n","    :return:\n","    train_dataloader\n","    validation_dataloader\n","    \"\"\"\n","\n","    with h5py.File(file_xtrain, \"r\") as fi:\n","        x_data = fi[derivation][()]\n","    y_data = pd.read_csv(file_ytrain)['sleep_stage'].to_numpy()\n","\n","    # Creating data indices for training and validation splits:\n","    dataset_size = len(y_data)\n","    indices = list(range(dataset_size))\n","    split = int((1 - validation_ratio) * dataset_size)\n","    np.random.shuffle(indices)\n","    train_indices, val_indices = indices[:split], indices[split:]\n","\n","    x_train, x_validation = x_data[train_indices], x_data[val_indices]\n","    y_train, y_validation = y_data[train_indices], y_data[val_indices]\n","\n","    # torch dataset\n","    train_dataset = EegEpochDataset(x_data=x_train, y_data=y_train)\n","    val_dataset = EegEpochDataset(x_data=x_validation, y_data=y_validation)\n","\n","\n","    return train_dataset, val_dataset\n","\n","\n","# load dataloaders - final_val is the dataset for the last validation\n","train_dataset, final_val_dataset = get_train_validation_dataset('eeg_5')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5fmw8OQxinj"},"source":["Now we create the neural network Model:\n","- convolutionnal neural network\n","- Fully conencted layers at the end\n","- takes only a single channel of EEG signal as input"]},{"cell_type":"code","metadata":{"trusted":true,"id":"jPAcxipDxinl"},"source":["import torch\n","import torch.nn as nn\n","\n","class SingleChannelConvNet(nn.Module):\n","\n","    def __init__(self):\n","        super(SingleChannelConvNet, self).__init__()\n","        # convolutionnal mayers\n","        self.conv_a = nn.Conv1d(1, 128, 7, stride=2, padding=6, padding_mode='zeros')\n","        self.conv_b = nn.Conv1d(128, 128, 7, stride=2, padding=6, padding_mode='zeros')\n","        self.conv_c = nn.Conv1d(128, 256, 7, stride=2, padding=6, padding_mode='zeros')\n","        self.conv_d = nn.Conv1d(256, 256, 5, stride=2, padding=4, padding_mode='zeros')\n","        self.conv_e = nn.Conv1d(256, 256, 3, stride=2, padding=2, padding_mode='zeros')\n","\n","        # pool layers\n","        self.pool = nn.MaxPool1d(2)\n","\n","        # non linearity\n","        self.activfunc_a = nn.LeakyReLU(negative_slope=0.1)\n","\n","        # fully connected layers - at the end\n","        self.fc1 = nn.Linear(3 * 256, 100)\n","        self.fc2 = nn.Linear(100, 5)\n","\n","    def forward(self, x):\n","\n","        x = self.activfunc_a(self.conv_a(x))\n","        for _ in range(5):\n","            x = self.activfunc_a(self.conv_b(x))\n","        x = self.activfunc_a(self.conv_c(x))\n","        for _ in range(3):\n","            x = self.activfunc_a(self.conv_d(x))\n","        x = self.activfunc_a(self.conv_e(x))\n","        x = self.activfunc_a(self.conv_e(x))\n","\n","        x = x.view(-1, self.num_flat_features(x)) # flatten the tensor\n","        x = self.activfunc_a(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYm_atwwxinp"},"source":["You can now start the training on the train dataloader:\n","- model will train many times on the dataset: n_epochs\n","- training dataset will be split in three subset (k_fold cross-validation)\n","- loss_val: mean loss on the validation datasets, computed after each epochs of training"]},{"cell_type":"code","metadata":{"trusted":true,"id":"R5puDba5xinp"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# device: use GPU if available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# parameters\n","learning_rate = 0.001\n","n_epoch = 20\n","k_fold = 3\n","batch_size = 32\n","\n","# neural network\n","my_net = SingleChannelConvNet()\n","my_net = my_net.to(device) # model into GPU\n","# loss function\n","criterion = nn.CrossEntropyLoss()\n","# optimisation algorithm \n","optimizer = optim.Adam(my_net.parameters(), lr=learning_rate)\n","\n","\n","# function: evaluate the loss of validation subset\n","def loss_val(net, val_loader):\n","    with torch.no_grad(): # do not forget to remove gradient computing during evaluation !!!\n","        val_loss = 0.0\n","        for data in val_dataloader:\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            \n","            outputs = net(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","        return val_loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7t3y4DlMxinu"},"source":["Let's start the loop !"]},{"cell_type":"code","metadata":{"trusted":true,"_kg_hide-output":true,"id":"_Vdufdptxinv"},"source":["from sklearn.model_selection import KFold\n","from torch.utils.data.dataset import Subset\n","from torch.utils.data import DataLoader\n","\n","# List all the validation loss:\n","# at the end of each epoch of training, a loss is computed on a subset of data\n","all_val_loss = []\n","\n","print('training...')\n","for epoch in range(n_epoch):  # loop over the dataset multiple times\n","\n","    # validation losses for this epoch (n=k_fold)\n","    val_loss = []        \n","    for train_indices, val_indices in KFold(n_splits=k_fold).split(list(range(len(train_dataset)))):\n","        # k_fold dataloader (k=3) - Take validation subset for training, to avoid overfit\n","        train_subset = Subset(train_dataset, train_indices)\n","        val_subset = Subset(train_dataset, val_indices)\n","\n","        train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=4)\n","        val_dataloader = DataLoader(val_subset, batch_size=batch_size, num_workers=8)\n","\n","        running_loss = 0.0\n","        for i, data in enumerate(train_dataloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + loss + backward + optimize\n","            outputs = my_net(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 100 == 99:\n","                print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss))\n","            running_loss = 0.0\n","\n","        # average validation losses\n","        val_loss += [loss_val(my_net, val_dataloader)]\n","        \n","    all_val_loss += [np.round(np.mean(val_loss), 2)]\n","    print(all_val_loss)\n","\n","    \n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"wTOU98Gixinz"},"source":["from pprint import pprint\n","from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n","\n","# score function\n","def evaluate(true, pred):\n","    scores = {'balanced_accuracy': balanced_accuracy_score(true, pred),\n","            'cohen_kappa': cohen_kappa_score(true, pred),\n","            'confusion_matrix': confusion_matrix(true, pred)}\n","\n","    return scores\n","\n","# params\n","classes = ['Wake', 'N1', 'N2', 'N3', 'REM']\n","\n","# final validation dataset: has not be used for the training\n","val_dataloader = DataLoader(final_val_dataset, batch_size=batch_size, num_workers=8)\n","\n","# evaluate the performance of the model\n","with torch.no_grad():\n","    prediction_list = torch.empty(0).to(device)\n","    true_list = torch.empty(0).to(device)\n","    for data in val_dataloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        \n","        outputs = my_net(inputs)\n","        _, predicted = torch.max(outputs, 1)\n","        prediction_list = torch.cat([prediction_list, predicted])\n","        true_list = torch.cat([true_list, labels])\n","\n","        \n","# Scores\n","true_list = true_list.cpu().numpy()\n","prediction_list = prediction_list.cpu().numpy()\n","scores = evaluate(true_list, prediction_list)\n","\n","print(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2G2pRC4Kxin3"},"source":["During the training, you may have noticed that you could have stopped earlier to have a lower validation, and maybe a better model at the end.\n","Rewrite the code to save the 3 models with the lower validation loss, and compare them on the final_validation_dataset !"]}]}